Resumo de Compiladores
Capítulo 1:
1.1 Compiladores
  Posto de forma simples, um compilador é um programa que lê um programa escrito
  numa linguagem - a linguagem fonte - e o traduz num programa equivalente numa
  outra linguagem - a linguagem alvo. Como importante parte desse processo de
  tradução, o compilador relata a seu usuário a presença de erros no programa
  fonte. Os compiladores são algumas vezes classificados como de uma passagem,
  de passagens múltiplas, de carregar e executar, depuradores ou otimizantes,
  dependendo de como tenham sido construídos ou que função se suponha que devam
  realizar. A despeito dessa aparente complexidade, as tarefas básicas que
  qualquer compilador precisa realizar são essencialmente as mesmas.

  - O Modelo de Compilação de Análise e Síntese:
  Existem duas partes na compilação: a análise e a síntese. A parte da análise
  divide o programa fonte nas partes constituintes e cria uma representação
  intermediária do mesmo. A de síntese constrói o programa alvo desejado, a
  partir da representação intermediária. Das duas, a síntese requer as técnicas
  mais especializadas. Durante a análise, as operações implicadas pelo programa
  fonte são determinadas e registradas numa estrutura hierárquica, chamada de
  árvore. Frequentemente, é utilizado um tipo especial de árvor, chamado árvore
  sintática, na qual cada nó representa uma operação e o filho de um nó
  representa o argumento da operação.

  Muitas ferramentas de software que manipulam programas fonte realizam algum
  tipo de análise. Alguns exemplos de tais ferramentas incluem:
    1. Editores de estruturas: Um editor de estruturas toma como entrada um
    conjunto de comandos para construir um programa fonte.
    2. Pretty printer: Um pretty printer analise um programa e o imprime numa
    forma em que a sua estrutura se torna claramente visível.
    3. Verificadores estáticos: Um verificador estático lê um programa,
    analisa-o e tenta descobrir seus erros pontenciais, sem executá-lo. A parte
    de análise é frequentemente similar àquela encontrada nos compiladores
    otimizantes.
    4. Interpretadores: Em lugar de produzir um programa alvo como resultado da
    tradução, um interpretador realiza operações especificadas pelo programa
    fonte. Os interpretadores são frequentemente usados para executar linguagens
    de comandos, dado que cada operador numa tal linguagem é usualmente uma
    invocação de uma rotina complexa, como um editor ou compilador.

  A parte de análise, em cada um dos seguintes exemplos seguintes, é similar
  aquela de um compilador convencional.
    1. Formatadores de texto: Um formatador de texto toma por entrada um fluxo
    de caracteres, a maior parte do mesmo como texto a ser composto
    tipograficamente, mas com alguma parte incluindo comandos, a fim de indicar
    parágrafos, figuras ou estruturas matemáticas, tais como subscritos e
    sobrescritos.
    2. Compiladores de silício: Um compilador de silício possui uma linguagem
    fonte que é similar ou idêntica à de uma linguagem de programação
    convencional. Entretanto as variáveis da mesma não representam localizações
    de memória mas sinais lógicos (0 ou 1) ou grupos de sinais de um circuito
    de chaveamento.
    3. Interpretadores de queries: Um interpretador de queries traduz um
    predicado, contendo operadores booleanos ou relacionais, em comandos, para
    percorrer um banco de dados, de forma a satisfazer ao predicado.

  - O Contexto de um compilador:
  Adicionalmente ao compilador, vários outros programas podem ser necessários
  para se criar um programa alvo executável. Um programa fonte pode ser divido
  em módulos armazenados em arquivos separados. A tarefa de coletar esses
  módulos é, algumas vezes, confiada a um programa distinto, chamado de
  pré-processador. O pré-processador pode, também, expandir formas curtas,
  chamadas de macros, em enunciados da linguagem fonte.

1.2 Análise do Programa Fonte
  Nesta seção, introduzimos a análise e ilustramos seu uso em algumas linguagens
  de formatação de texto. Na compilação, a análise consiste em três fases:
  1. Análise linear, na qual um fluxo de caracteres constituindo um programa é
  lido da esquerda para a direita e agrupado em tokens, que são sequencias de
  caracteres tendo um significado coletivo.
  2. Análise hierárquica, na qual os caracteres ou tokens são agrupados
  hierarquicamente em coleções aninhadas com significado coletivo.
  3. Análise semântica, na qual certas verificações são realizadas a fim de se
  assegurar que os componentes de um programa se combinam de forma
  significativa.

  - Análise Léxica
  Num compilador, a análise linear é chamada e análise léxica ou
  esquadrinhamento (scanning). Por exemplo, na análise léxica, os caracateres
  no enunciado de atribuição
  montante := deposito_inicial + taxa_de_juros * 60
  poderiam ser agrupados nos seguintes tokens:
  1. O identificador montante
  2. O símbolo de atribuição :=
  3. O identificador deposito_inicial
  4. O sinal de adição
  5. O identificador taxa_de_juros
  6. O sinal de multiplicação
  7. O número 60.
  Os espaçoes que separam os caracteres desses tokens seriam normalmente
  eliminados durante a análise léxica.

  - Análise Sintática
  A análise hierárquica é chamada de análise gramatical ou análise sintática.
  Envolve o agrupamento dos tokens do programa fonte em frases gramaticais, que
  são usadas pelo compilador, a fim de sintetirar a saída. Usualmente, as frases
  gramaticais do programa fonte são representadas por uma árvore gramatical.
  A estrutura hierárquica de um programa é usualmente expressa por regras
  recursivas. A divisão entre a análise léxica e sintática é um tanto
  arbitrária. Usualmente, escolhemos uma que simplifique a tarefa global de
  análise. Um fator determinante na divisão é o de uma construção da linguagem
  fonte ser inerentemente recursiva ou não.

  - Análise Semântica
  A fase de análise semântica verifica os erros semânticos no programa fonte e
  captura as informações de tipo para a fase subsequente de geração de código.
  Utiliza a estrutura hierárquica determinada pela fase de análise sintática,
  a fim de identificar os operadores e operandos das expressões e enunciados.
  Um importante componente da análise semântica é a verificação de tipos. Nela
  o compilador checa se cada operador recebe os operandos que são permitidos
  pela especificação da linguagem fonte.

  - A Anáselise nos Formatadores de Texto
  É útil considerar a entrada para um formatador de texto como especificando
  uma hierarquia de compartimentos, que são reguiões retangulares a serem
  preenchidas por algum padrão de bits, representando pontos claros e escuros a
  serem impressos no dispositivo de saída.

1.3 As Fases de Um Compilador
  Conceitualmente, um compilador opera em fases, cada uma das quais transforma o
  programa fonte de uma representação para outra. As três primeiras fases,
  formando o núcleo da parte de análise do compilador, foram introduzidas na
  última seção. Duas outras atividades, o gerenciamento da tabela de símbolos e
  a manipulação de erros, são mostradas interagindo com as seis fases de analise
  léxica, análise sintática, análise semântica, geração de código intermediário,
  otimização e geração de código. Informalmente, também chamaremos de "fases" o
  gerenciador da tabela de símbolos e o manipulador de erros.

  - Gerenciamento da Tabela de Símbolos
  Uma função essencial do compilador é registrar os identificadores usados no
  programa fonte e coletar as informações sobre os seus diversos atributos.
  Esses atributos podem providenciar informações sobre a memória reservada
  para o identificador, seu tipo, escopo (onde é válido no programa) e, no caso
  de nomes de procedimentos, coisas tais como o número e os tipos de seus
  argumentos, o método de transmissão de cada um (por exemplo, por referência) e
  o tipo retornado, se algum.
  Uma tabela de símbolos é uma estrutura de dados contendo um registro para cada
  identificador, com os campos contendo os atributos do identificador. A
  estrutura de dados nos permite encontrar rapidamente cada registro e,
  igualmente, armazenar ou recuperar dados do mesmo.

  - Detecção de Erros e Geração de Relatórios
  Cada fase pode encontrar erros. Entretanto, apóes encontrá-los, precisa lidar
  de alguma forma com os mesmos, de tal forma que a compilação possa continuar,
  permitindo que sejam detectados erros posteriores no programa fonte. Um
  compilador que pare ao encontrar o primeiro erro não é tão prestativo quanto
  poderia sê-lo. As fases de análise sintática e semântica tratam usualmente de
  uma ampla fatia dos erros detectáveis pelo compilador. A fase de análise
  léxica pode detectá-los quando os caracteres remanescentes na entrada não
  formem qualquer token da linguagem. Os erros, onde o fluxo de tokens viole as
  regras estruturais (sintaxe) da linguagem sãoo determinados pela fase de
  análise sintática. Durante a análise semântica, o compilador tenta detectar as
  construções que possuam a estrutura sintática correta, sem nenhuma preocupação
  com o significado da operação envolvida.

  - As Fases de Análise
  À medida que a tradução progride, a representação interna do compilador para
  programa fonte muda. A fase de análise léxica lê os caracteres de um programa
  fonte e os agrupa num fluxo de tokens, no qual cada token representa uma
  sequência de caracteres logicamente coesiva, como, por exemplo, um
  identificador, uma palavra chave (if, while, etc), um caractere de pontuação
  ou um operador composto por vários caracteres (:=). A sequência dos caracteres
  que formam um token é chamado o lexema para aquele token.
  Certos tokens serão enriquecidos por um "valor léxico". A análise sintática
  impõe uma estrutura hierárquica ao fluxo de tokens, a qual iremos retratar
  através de árvores sintáticas.

  - Geração de Código Intermediário
  Após as análises sintática e semântica, alguns compiladores geram uma
  representação intermediária explícita do programa fonte. Podemos pensar dessa
  representação intermediária como um programa para uma máquina abstrata. Essa
  representação intermediária deverias possuir duas propriedades importantes:
  ser fácil de produzir e fácil de traduzir no programa alvo. A representação
  intermediária pode ter uma variedade de formas, consideremos uma forma
  intermediária chamada "código de três endereços", que é como uma linguagem
  de montagem para uma máquina, na qual cada localização de memória possa atuar
  como um registrador. O código de três endereços consiste em uma sequência de
  instruções, cada uma delas possuindo no máximo três operandos.

  - Otimização de Código
  A fase de otimização tenta melhorar o código intermediário, de tal forma que
  venha resultar um código de máquina mais rápido em tempo de execução. Algumas
  otimizações são triviais. Existe uma grande variação na quantidade de
  otimizações de código que cada compilador executa. Naqueles que mais a
  realizam, chamados de "compiladores otimizantes", uma porção significativa
  de seus tempos é gasta nessa fase.

  - Geração de Código
  A fase final do compilador é a geração do código alvo, consistindo normalmente
  de código de máquina relocável ou código de montagem. As localizações de
  memória são selecionadas para cada uma das variáveis usadas pelo programa.
  Então, as intruções intermediárias são, cada uma, traduzidas numa sequência de
  instruções de máquina que realizam a mesma tarefa. Um aspecto crucial é a
  atribuição das variáveis aos registradores.

1.4 Os Primos do Compilador
  A entrada para o compilador pode ser produzida por um ou mais
  pré-processadores e pode ser necessário processamento posterior da saida do
  compilador, antes do código de máquina ser obtido. Nessa seção, discutimos
  o contexto no qual um compilador tipicamente opera.

  - Pré-processadores
  Os pré-processadores produzem entrada para compiladores. Podem realizar as
  as seguintes funções:
  1. Processamento de macros: Um pré-processador pode permitir que um usuário
  defina macros que sejam abreviações para construções mais longas.
  2. Inclusão de arquivos: Um pré-processador pode incluir arquivos no papel de
  cabeçalhos do texto do programa. Por exemplo, o pré-processador C faz com que
  o conteúdo do arquivo global <global.h> substitua o enunciado
  #include <global.h> ao processar um arquivo contendo tal enunciado.
  3. Pré-processadores "racionais": Tais pré-processadores expandem as
  linguagens mais antigas com facilidades modernas de controle de fluxo e
  estruturação de dados.
  4. Extensores de linguagens: Tentam conferir maior poder às linguagens,
  atráves de macros embutidas.

1.5 O agrupamento das Fases
  A discussão das fases na Seção 1.3 lida com a organização lógica do
  compilador. Numa implementação, as atividades de mais de uma fase são
  frequentemente agrupadas.

  - Interfaces de Vanguarda e Retaguarda
  Frenquentemente, as fases são coletadas numa interface de vanguarda ou de
  retarguarda. A interface de vanguarda consiste naquelas fases, ou partes de
  fases, que dependem primariamente da linguagem fonte e são amplamente
  independentes da máquina alvo. Dentre essas fases são normalmente incluídas
  a análise léxica e sintática, a criação da tabela de símbolos, a análise
  semântica, e a geração de código intermediário. Uma certa quantidade de
  otimizações de código pode ser feita igualmente pela interface de vanguarda.
  A interface de vanguarda também inclui o tratamento de erros que está
  associado a essas fases.
  A interface de retaguarda inclui aquelas partes do compilador que dependem da
  máquina alvo e que, geralmente, não dependem da linguagem fonte, tão-só da
  linguagem intermediária. Na interface de retaguarda encontramos alguns
  aspectos das fases de otimização e de geração de código, juntamente com as
  operações de tratamento de erro e manipulação da tabela de símbolos
  necessárias.

1.6 Ferramentas para a Construção de Compiladores
  O escritor de um compilador, como qualquer outro programador, pode usar,
  vantajosamente, ferramentas de software, tais como depuradores, gerenciadores
  de versões, customizadores e assim por diante. Logo após a escrita dos
  primeiros compiladores, surgiram os sistemas para auxiliar esse processo.
  Foram frequentemente referidos como compiladoes de compiladores, geradores de
  compiladores e sistemas de escrita de tradutores. São amplamente orientados
  em torno de um modelo particular de linguagem e mais adequados para gerar
  compiladores de linguagens similares ao modelo. O que se segue é uma lista de
  algumas ferramentas úteis para a construção de compiladores:
  1. Geradores de analisadores gramaticais.
  2. Geradores de analisadores léxicos.
  3. Dispositivos de tradução dirigida pela sintaxe.
  4. Geradores automáticos de código.
  5. Dispositivos de fluxo de dados.

Capítulo 2:
2.1 Visão Geral
  Uma linguagem de programação pode ser definida pela descrição da aparência de
  seus programas (a sintaxe da linguagem) e do que os mesmos significam (a
  semântica da linguagem). Para especificar a sintaxe de uma linguagem,
  apresentamos uma notação amplamente aceita, chamada de gramática livre de
  contexto ou BNF (para Forma de Backus-Naur). Com as notações correntemente
  disponíveis, a semântica é muito mais difícil de se descrever do que a
  sintaxe. Consequentemente, para especificar a semântica de uma linguagem
  usaremos descrições informais e exemplos sugestivos. Além de especificar a
  sintaxe da linguagem, uma gramática livre de contexto pode ser usada como
  auxílio para guiar a tradução de programas. Uma técnica de compilação,
  orientada por gramáticas, conhecida como tradução dirigida pela sintaxe, é de
  muita ajuda na organização das partes da vanguarda do compilador.

2.2 Definição da Sintaxe
  Uma gramática descreve naturalmente a estrutura hierárquica de muitas
  construções das linguagens de programação. Por exemplo, um comando if-else, em
  C, possui a forma:
  if (expressão) comando else comando
  Ou seja, o comando é uma concatenação da palavra-chave if, um parênteses à
  esquerda, uma expressão, um parênteses à direita, um comando, a palavra-chave
  else e um outro comando. Tal regra é chamada de uma produção. Numa produção,
  os elementos léxicos, como a palavra-chave if e os parênteses, são chamados de
  tokens. As variávies como expressão e comando, representam sequências de
  tokens e são chamadas de não-terminais.
  Uma gramática livre de contexto possui quatro componentes:
  1. Um conjunto de tokes, conhecidos como símbolos terminais.
  2. Um conjunto de não-terminais.
  3. Um conjunto de produções, onde uma produção consiste em um não-terminal,
  chamado de lado esquerdo da produção, uma seta e uma sequência de tokens e/ou
  não-terminais, chamados de lado direito da produção.
  4. Uma designação a um dos não-terminais como o símbolo de partida.
  Seguimos a convenção de especificar gramáticas pela listagem de suas
  produções, com aquelas para o símbolo de partida figurando à frente das
  demais. Assumimos que os dígitos, os sinais, tais como <=, e as cadeias de
  caracteres em negrito, como while, sejam terminais. Um nome em itálico é um
  não-terminal e qualquer nome ou símbolo que não esteja em itálico deve ser
  assumido como um token.
  Dizemos que uma produção é para um não-terminal se o último figurar no lado
  esquerdo da primeira. Uma cadeia de tokens é uma sequência de zero ou mais
  tokens. A cadeia contendo zero tokens, escrita E, é chamada de cadeia vazia.
  Uma gramática deriva cadeias começando pelo símbolo de partida e, então,
  substituindo repetidamente um não-terminal pelo lado direito de uma produção
  para aquele não-terminal. As cadeias de tokens que podem ser derivadas a
  partir do símbolo de partida formam a linguagem definida pela gramática.

  - Árvores Gramaticais
  Uma árvore gramatical mostra, pictoricamente, como o símbolo de partida de uma
  gramática deriva uma cadeia de linguagem. Se um não terminal A possui uma
  produção A -> XYZ, então uma árvore gramatical pode ter um nó interior
  rotulado A, com três filhos rotulados X, Y e Z, da esquerda para direita.
  Formalmente, dada uma gramática livre de contexto, uma árvore gramatical
  possui as seguintes propriedades:
  1. A raiz é rotulada pelo símbolo de partida.
  2. Cada folha é rotulada por um token ou por E (cadeia vazia).
  3. Cada nó interior é rotulado por um não-terminal.
  4. Se A é um não-terminal rotulando algum nó interior e X1, X2, ..., Xn são
  rótulos dos filhos daquele nó, da esquerda para a direita, então
  A ->X1X2...Xn é uma produção. Aqui X1X2...Xn figuram no lugar de símbolos
  que sejam terminais ou não-terminais. Como um caso especial, se A -> E, então
  um nó rotulado A deve possuir um único filho rotulado E (cadeia vazia).

  As folhas da árvore gramatical, lidas da esquerda para a direita, formam o
  produto da árvore, que é a cadeia gerada ou derivada a partir do não-terminal
  à raiz da árvore gramatical. Outra definição da linguagem gerada por uma
  gramática é a do conjunto de cadeias que podem ser geradas por alguma árvore
  gramatical. O processo de encontrar uma árvore gramatical para uma dada cadeia
  de tokens é chamado de análise gramatical ou análise sintática daquela cadeia.

  - Ambiguidade
  Temos que ser cuidadosos ao falar sobre a estrutura de uma cadeia segundo uma
  gramática. Conquanto seja claro que cada árvore gramatical dê origem
  exatamente à cadeia formada por suas folhas, uma gramática pode ter mais de
  uma árvore gramatical gerando uma dada cadeia de tokens. Tal gramática é dita
  ambígua.

  - Associatividade dos Operadores
  Convecionalmente, 9+5+2 é equivalemte a (9+5)+2 e 9-5-2 a (9-5)-2. Quando um
  operando, como 5, possui operadores à esquerda e à direita, são necessárias
  converções para decidir que operador recebe que operando. Dizemos que o
  operador + associa à esquerda poruqe um operando com sinais de adição em ambos
  os lados é absorvido pelo operador à sua esquerda. Na maioria das linguagens
  de programação, os quatro operadores aritméticos (adição, subtração,
  multiplicação e divisão) são associativos à esquerda. Alguns operadores
  comuns, tais como exponenciação, são associativos à direita. Como outro
  exemplo, o operador de atribuição (=) em C é associativo à direita.

  - Precedência de Operadores
  Considere a expressão 9+5*2. Existem duas possíveis interpretações para a
  mesma: (9+5)*2 ou 9+(5*2). A associatividade de + e de * não resolve essa
  ambiguidade. Por esse motivo, precisamos conhecer a precedência relativa dos
  operadores quando mais de um tipo deles estiver presente. Dizemos que o *
  possui precedência mais alta do que +, se * capturar seus operandos antes de
  + o fazer. Na aritmética ordinária, a multiplicação e a divisão têm maior
  precedência que a adição e a subtração.
  Sintaxe das expressões. Uma gramática para expressões aritméticas pode ser
  construída a partir de uma tabela mostrando a associatividade e a precedência
  dos operadores.

2.4 A Análise Gramatical
  A análise gramatical é o processo de se determinar se uma cadeia de tokens
  pode ser gerada por uma gramática. Na discussão do problema, serve como apoio
  ao raciocínio pensar que uma árvore gramatical está sendo construída, ainda
  que um compilador não construa efetivamente. No entanto, um analisador
  gramatical precisa ser capaz de construir uma árvore ou então a compilação não
  poderá ser garantida correta. Uma alternativa viável é a de usar uma
  ferramenta de software a fim de gerar um tradutor diretamente a partir de um
  esquema de tradução.
  Um analisador gramatical pode ser construído para qualquer gramática. As
  gramáticas usadas na prática, entretanto, possuem uma forma especial. Para
  qualquer gramática livre de contexto existe um analisador gramatical que toma
  no maximo um tempo proporcional a O(n^3) para analisar gramaticalmente uma
  cadeia de n tokens. Mas tempo ao cubo é muito caro. Dada uma linguagem de
  programação, podemos geralmente construir uma gramática que possa ser
  analisada de forma rápida. A maioria dos métodos de análise gramatical cai em
  uma dentre duas classes, chamadas de top-down e bottom-up. Esses termos se
  referem à ordem na qual os nós da árvore gramatical são construídos. No
  primeiro, a construção se inicia na raiz e prossegue em direção às folhas,
  enquanto que no último, a construção se inicia nas folhas e procede em direção
  à raiz.

  - Análise Gramatical Preditiva
  A análise gramatical descendente recursiva é um método top-down de análise
  sintática, no qual executamos um conjunto de procedimentos recursivos para
  processar a entrada. Um procedimento é associado a cada não-terminal de uma
  gramática. Aqui, consideramos uma forma especial de análise gramatical
  descendente recursiva, chamada de análise gramatical preditiva, na qual o
  símbolo lookahead determina inambiguamente o procedimento selecionado para
  cada não-terminal. A sequência de procedimentos chamada no processamento da
  entrada define implicitamente uma árvore gramatical para a entrada.

  - Quando Usar Produções-E
  As produções com E no lado direito requerem tratamento especial. O analisador
  gramatical descendente recursivo usa uma produção-E como um default, quando
  nenhum outra puder ser usada.

2.6 Análise Léxica
  Iremos agora adicionar ao tradutor da secção prévia um analisador léxico que
  lê e converte a entrada para um fluxo de tokens a ser analisado pelo parser.
  Uma sequência de caracteres de entrada que compõe um único token é chamada de
  lexema. Um scanner (analisador léxico) pode isolar o parser da representação
  sob a forma de lexema dos tokens. Começamos por listar algumas das funções que
  gostaríamos que um analisador léxico realizasse.
  Remoção de Espaçoes em Branco e Comentários: O tradutor de expressões examina
  cada caractere da entrada, de tal forma que caracteres estranhos, como
  brancos, irão fazer com que o mesmo falhe. Muitas linguagens permitem que
  "espaços em branco" (brancos, tabulações e avanço de linha) apareçam entre os
  tokens. Os comentários podem igualmente ser ignorados pelo parser e tradutor,
  de tal forma que pode ser tratados também como espaços em branco.
  Constantes: A qualquer tempo em que um dígito isolado apareça numa expressão,
  parece razoável admitar uma constante inteira arbitrária no seu lugar. Como
  uma constante inteira é uma sequência de dígitos, as constantes inteiras podem
  ser admitidas pela adição à gramática de produções para expressões ou criando
  um token para tais constantes.
  Reconhecendo Identificadores e Palavras-Chave: As linguagens usam
  identificadores para nomes de variáveis, de arrays, e para outros elementos.
  Uma gramática para uma linguagem frequentemente trata um identificador como um
  token. Um parser baseado numa tal gramática pretende ver o mesmo token,
  digamos id, a cada vez que um identificador aparecer na entrada.

  - Interface para o Analisador Léxico
  Quando um analisador léxico é inserido entre o parser e o fluxo de entrada, o
  mesmo interage com os dois, lê os caracteres de entrada, agrupa-os em lexemas
  e passa os tokens formados pelos lexemas, juntamente com os valores dos seus
  atributos, para os estágios posteriores do compilador. Em algumas situações o
  analisador léxico tem que ler alguns caracteres à frente antes de estar apto a
  decidir a respeito do token a ser retornado para o parser. O analisador léxico
  e o parser formam um par produtor-consumidor. O analisador léxico produz os
  tokens e o parser os consome. Os tokens produzidos podem ser guardados num
  buffer até que venham a ser consumidos. A interação entre os dois é
  restringida somente pelo tamanho do buffer, por que o analisador léxido não
  poderá prosseguir quando o buffer estiver cheio e o parser quando o buffer
  estiver vazio.

Capítulo 3:
  Este capítulo lida com as técnicas para especificar e implementar analisadores
  léxicos. Uma forma simples de se construir um analisador léxico é escrever um
  diagrama que ilustre a estrutura dos tokens da linguagem-fonte e então
  traduzi-lo manualmente num programa que os localize. Analisadores léxicos
  eficientes podem ser produzidos dessa forma.

  3.1 O Papel do Analisador Léxico
  O analisador léxico é a primeira fase de um compilador. Sua tarefa principal
  é a de ler os caracteres de entrada e produzir uma sequência de tokens que o
  parser utiliza para a análise sintática. Essa interação é comumente
  implementada fazendo-se com que o analisador léxico seja uma sub-rotina ou
  uma co-rotina do parser. Ao receber do parser um comando "obter o próximo
  token", o analisador léxico lê os caracteres de entrada até que possa
  identificar o próximo token. Como o analisador léxico é a parte do compilador
  que lê o texto-fonte, também pode realizar algumas tarefas secundárias ao
  nível da interface com o usuário. Uma dela é a de correlacionar as mensagens
  de erro do compilador com o programa-fonte.

  - Temas da Análisa Léxica
  Existem várias razões para se dividir a fase de análise da compilação em
  análise léxica e análise gramatical (parsing).
  1. Um projeto mais simples talvez seja a consideração mais importante. A
  separação das análises léxica e sintática frequentemente nos permite
  simplificar uma ou outra dessas fases.
  2. A eficiência do compilador é melhorada. Um analisador léxico separado nos
  permite construir um processador especializado e potencialmente mais eficiente
  para a tarefa.Uma grande quantidade de tempo é gasta lendo-se o programa-fonte
  e particionando-o em tokens.
  3. A portabilidade do compilador é realçada. As peculiaridades do alfabeto de
  entrada e outra anomalias específicas de dispositivos podem ser restringidas
  ao analisador léxico.

  - Tokens, Padrões, Lexemas
  Quando se fala sobre a análise léxica, usamos os termos "token", "padrão" e
  "lexema" com significados específicos. Em geral, existe um conjunto de cadeias
  de entrada para as quais o mesmo token é produzido como saída. Esse conjunto
  de cadeias é descrito por uma regra chamada de um padrão associado ao token de
  entrada. O padrão é dito reconhecer cada cadeia do conjunto. Um lexema é um
  conjunto de caracteres no programa-fonte que é reconhecido pelo padrão de
  algum token.
  Tratamos os tokens como símbolos terminais na gramática para a
  linguagem-fonte, usando nomes em negrito para representá-los Os lexemas
  reconhecidos pelo padrão do token representam cadeias de caracteres no
  programa-fonte, e podem receber um tratamento conjunto, como instâncias de uma
  mesma unidade léxica (por exemplo, instâncias de identificadores, números
  etc.). Na maioria das linguagens de programação, as seguintes construções são
  tratadas como tokens: palavras-chave, operadores, identificadores, constantes,
  literais, cadeias e símbolos de pontuação, como parênteses, vírgulas e
  ponto-e-vírgulas.

  - Atributos para os tokens
  Quando um lexema for reconhecido por mais de um padrão, o analisador léxico
  precisará providenciar informações adicionais para as fases subsequentes do
  compilador a respeito do lexema particular que foi reconhecido. Por exemplo, o
  padrão num reconhece as duas cadeias 0 e 1, mas é essencial para o gerador de
  código ser informado sobre que cadeia foi efetivamente reconhecida.
  O analisador léxico coleta informações a respeito dos tokens em seus atributos
  associados. Os tokens influenciam decisões na análise gramatical; os atributos
  influenciam a tradução dos tokens.

  - Erros Léxicos
  Poucos erros são distinguíveis somente no nível léxico, uma vez que um
  analisador léxico possui uma visão muito local do programa-fonte. Suponhamos
  que emerja uma situação na qual o analisador léxico seja incapaz de
  prosseguir, por que nenhum dos padrões reconheça um prefixo na entrada
  remanescente. Talvez a estratégia mais simples de recuperação seja a da
  "modalidade pânico". Removemos sucessivos caracteres de entrada remanescente
  até que o analisador léxico possa encontra um token bem-formado. Essa técnica
  de recuperação pode ocasionalmente confundir o parser, mas num ambiente de
  computação interativo pode ser razoavelmente adequada.
  Outras possíves ações de recuperações de erros são:
  1. remover um caractere estranho
  2. inserir um caractere ausente
  3. substituir um caractere incorreto por um correto
  4. transpor dois caracteres adjacentes.
  Transformaçẽos de erros como essas podem ser experimentadas numa tentativa de
  se consertar a entrada. A mais simples de tais estratégias é a de verificar se
  um prefixo da entrada remanescente pode ser transformado num lexema válido
  atráves de uma única transformação.

  3.3 Especificação dos Tokens
  As expressões regulares são uma notação importante para especificar padrões.
  Cada padrão corresponde a um conjunto de cadeias e, dessa forma, as expressões
  regulares servirão como nomes para conjuntos de cadeias.

  - Cadeias e Linguagens
  O termo alfabeto ou classe de caracteres denota qualquer conjunto finito de
  símbolos. Exemplos típicos de símbolos são letras e caracteres. O conjunto
  {0, 1} é o alfabeto binário; EBCID e ASCII são dois exemplos de alfabetos de
  computadores. Uma cadeia sobre algum alfabeto é uma sequência finita de
  símbolos retirados do mesmo. Na teoria das linguagens, os termos sentença e
  palavra são frequentemente usados como sinônimos para o termo "cadeia". O
  comprimento da cadeia s, usualmente escrito |s|, é o núemro de ocorrências de
  símbolos em s. A cadeia vazia, denotada E, é uma cadeia especial de
  comprimento zero.
  O termo linguagem denota qualquer conjunto de cadeia sobre algum alfabeto
  fixo. A definição é muito ampla. Linguagens abstratas como o conjunto vazio,
  ou {E}, o conjunto contendo somente a cadeia vazia, são linguagens sob essa
  definição. Também o são o conjunto de todos os programas Pascal sintaticamente
  bem formados, e o conjunto de todas as sentenças gramaticalmente corretas em
  inglês, apesar dos dois últimos conjuntos serem muito difíceis de se
  especificar.

  - Operações em Linguagens
  Existem várias operações importantes que podem ser aplicadas às linguagens.
  Para a análise léxica, estaremos primeiramente interessados na união,
  concatenação e fechamento. Podemos também generalizar o operador de
  "exponenciação" para linguagens, definindo-se L^0 sendo {E} e L^i como
  L^i-1*L. Desa forma L^i é L concatenado consigo mesma i-1 vezes.

  - Expressões Regulares
  Nesta seção, apresentamos uma notação, chamada de expressões regulares, que
  nos permite definir precisamente conjuntos de diversas naturezas. Uma
  expressão regular é constituída de expressões regulares mais simples usando-se
  um conjunto de regras de definição. Cada expressão regular r denota uma
  linguagem L(r). As regras de definição especificam como L(r) é formada através
  da combinação, em várias formas, das linguagens denotas pelas subexpressões de
  r.

  - Definições Regulares
  Por uma conveniência de notação, podemos deseja dar nomes a expressões
  regulares, bem como definir outras, usando esses nomes como se fossem
  símbolos.

  - Simplificações Notacionais
  Algumas construções ocorrem tão frequentemente nas expressões regulares que é
  conveniente introduzir algumas simplificações notacionais para as mesmas.
  1. Uma ou mais ocorrências. O operador unário pós-fixo * significa "uma ou
  mais ocorrências de".
  2. Zero ou mais ocorrências. O operador pós-fixo unário ? significa "zero ou
  uma ocorrência de".
  3. Classes de caracteres. A notação [abc], onde a, b e c são símbolos de
  alfabeto, denota a expressão regular a | b | c. Uma classe de caracteres
  abreviada, tal como [a-z] denota a expressão regular a | b | ... | z.

  - Conjuntos Não-regulares
  Algumas linguagens não podem ser descritas por qualquer expressão regular.

Capítulo 4:
  Cada linguagem de programação possui as regras que descrevem a estrutura
  sintática dos programas bem-formados. A sintaxe das construções de uma
  linguagem de programação pode ser descrita pelas gramáticas livres de contexto
  ou pela notação BNF (Forma de Backus-Naur).
  - Uma gramática oferece, para uma linguagem de programação, uma especificação
  sintática precisa e fácil de entender.
  - Para certas classes de gramáticas, podemos construir automaticamente um
  analisador sintático que determine se um programa-fonte está sintaticamente
  bem-formado.
  - Uma gramática propriamente projetada implica uma estrutura de linguagem de
  programação útil à tradução correta de programas-fonte em códigos-objeto e
  também à detecção de erros.
  - As linguagens evoluíram ao longo de um certo período de tempo, adquirindo
  novas construções e realizando tarefas adicionais.

  O núcleo deste capítulo está devotado aos métodos de análise sintática que são
  tipicamente usados nos compiladores.

  4.1 O Papel do Analisador Sintático
  Em nosso modelo de compilador, o analisador sintático obtém uma cadeia de
  tokens proveniente do analisador léxico e verifica se a mesma pode ser gerada
  pela gramática da linguagem-fonte. Esperamos que o analisador sintático relate
  quaisquer erros de sintaxe de uma forma inteligível. Deve também se recuperar
  dos erros que ocorram mais comumente, a fim de poder continuar processando
  o resto de sua entrada.
  Existem três tipos gerais de analisadores sintátidos. Os métodos universais de
  análise sintática, tais como o algoritmo de Cocke-Younger-Kasami e o de
  Earley, podem tratar qualquer gramática. Esses métodos, entretanto, são muito
  ineficientes para se usar num compilador de produção. Os métodos mais
  comumente usados nos compiladores são classificados como top-down ou
  bottom-up. Como indicado por seus nomes, os analisadores top-down constroem
  árvores do topo (raiz) para o fundo (folhas), enquanto os bottom-up começam
  pelas folhas e trabalham árvore acima até a raiz. Em ambos os casos, a entrada
  é varrida da esquerda para direita, um símbolo de cada vez.

  - Tratamento dos Erros de Sintaxe
  Sabemos que os programas podem conter erros em muitos níveis diferentes. Por
  exemplo, os erros podem ser:
    - léxicos, tais como errar a grafia de um identificador, palavra-chave ou
    operador
    - sintáticos, tais como uma expressão aritmética com parênteses
    não-balanceados
    - semânticos, tais como um operador aplicado a um operando incompatível
    - lógicos, tais como uma chamada infinitamente recursiva.
  Frequentemente, boa parte da detecção e recuperação de erros num compilador
  gira em torno da fase de análise sintática. Isto porque os erros ou são
  sintáticos por natureza ou são expostos quando o fluxo de tokens proveniente
  do analisador léxico desobedece às regras gramaticais que definem a linguagem
  de programação.
  O tratador de erros num analisador sintático possui metas simples de serem
  estabelecidas:
    - Deve relatar a presença de erros clara e acuradamente.
    - Deve se recuperar de cada erro suficientemente rápido a fim de ser capaz
    de detectar erros subsequentes.
    - Não deve retardar significativamente o processamento de programas
    corretos.
  A realização efetiva dessas metas apresenta desafios difíceis.
  Felizmente, os erros comuns são simples e frequentemente basta um mecanismo de
  tratamento de erros relativamente direto. Em alguns casos, entretanto, um erro
  pode ter ocorrido muito antes de sua presença ter sido detectada e sua
  natureza precisa pode ser muito difícil de ser deduzida.

  - Estratégias de Recperação de Erros
  Existem muitas estratégias gerais diferentes que um analisador sintático pode
  empregar para se recuperar de um erro sintático. Apesar de nenhuma delas ter
  provado ser universalmente aceitável, uns poucos métods têm ampla
  aplicabilidade. Introduzimos aqui as seguintes estratégias:
    - modalidade do desespero: este é o método mais simples de implementar. Ao
    descobrir um erro, o analisador sintático descarta símbolos de entrada, um
    de cada vez, até que seja encontrado um token pertencente a um conjunto
    designado de tokens de sincronização.
    - nível de frase: ao descobrir um erro, o analisador sintático pode realizar
    uma correção local na entrada restante. Isto é, pode substituir um prefixo
    de entrada remanescente por alguma cadeia que permita ao analisador seguir
    em frente.
    - produções de erro: se uma produção de erro for usada pelo analisador,
    podemos gerar diagnósticos apropriados para indicar a construção ilegal que
    foi reconhecida na entrada.
    - correção global: algortimos para escolhar uma sequencia mínima de mudanças
    de forma a se obter uma correção global de menor custo.

  4.2 Gramáticas Livres de Contexto
  Muitas construções de linguagens de programação possuem uma estrutura
  inerentemente recursiva que pode ser identificada por gramáticas livres de
  contexto. Uma gramática livre de contexto (gramática, simplificadamente)
  consiste em terminais, não-terminais, um símbolo de partida e produções.
  1. Os terminais são símbolos básicos a partir dos quais as cadeias são
  formadas. A palavra "token" será um sinônimo de "terminal" ao falarmos a
  respeito de gramáticas para linguagens de programação. Cada palavra-chave
  if, then e else é um terminal.
  2. Os não-terminais são variáveis sintáticas que denotam cadeias de
  caracteres. cmd e expr são não terminais. Os não-terminais definem conjuntos
  de cadeias que auxiliam a definição da linguagem gerada pela gramática.
  3. Numa gramática, um não-terminal é distinguido como o símbolo de partida, e
  o conjunto de cadeias que o mesmo denota é a linguagem definida pela
  gramática.
  4. As produções de uma gramática especificam a forma pela qual os terminais e
  não-terminais podem ser combinados a fim de formar cadeias. Cada produção
  consiste em um não-terminal, seguido por uma seta, seguido por uma cadeia de
  não-terminais e terminais.

  - Derivações
  Existem várias formas de se enxergar o processo pelo qual uma gramática define
  uma linguagem. A visão derivacional fornece uma precisa descrição da
  construção top-down da árvore gramatical. A ideia central aqui é que uma
  produção seja tratada como uma regra de reescrita, na qual o não-terminal à
  esquerda é substituído pela cadeia no lado direito da produção.

  - Árvores Gramaticais e Derivações
  Uma árvore gramatical pode ser vista como uma representação gŕafica para uma
  derivação que filtre a escolha relacionada à ordem de substituição. Cada nó
  interior de uma árvore gramatical é rotulado por algum não-terminal A e que os
  folhos de um nó são rotulados, da esquerda para a direita, pelos símbolos do
  lado direito da produção pelos quais A foi substituído na derivação. As folhas
  da árvore gramatical são rotuladas por não-terminais ou terminais e, lidos da
  esquerda para direita, constituem uma forma sentencial chamada de produto ou
  fronteira da árvore.

  - Ambiguidade
  Uma gramática que produza mais de uma árvore gramatical para alguma setença
  é dita ambígua. Colocado de outra forma, uma gramática ambígua é aquela que
  produz mais de uma derivação à esquerda, ou à direita para a mesma sentença.
  Para certos tipos de analisadores sintáticos, é desejável que a gramática seja
  inambígua, porque se não o for, não poderemos selecionar, de forma única, a
  árvore gramatical para uma dada sentança.

  4.3 Escrevendo Uma Gramática
  As gramáticas são capazes de descrever a maioria, mas não a totalidade das
  sintaxes das linguagens de programação. Uma parte limitada da análise
  sintática é realizada pelo analisador léxico, na medida em que produz uma
  sequência de tokens a partir dos caracteres de entrada. Certas restrições
  feitas à entrada, tais como a exigência de que os identificadores sejam
  declarados antes de serem usados, não podem ser descritas por uma gramática
  livre de contexto. Consequentemente, as sequencias de tokens aceitas por um
  analisador sintático forma um subconjunto de uma linguagem de programação.

  - Expressões Regulares vs. Gramáticas Livres de Contexto
  Cada construção que possa ser descrita por uma expressão regular também pode
  ser descrita por uma gramática. Uma vez que cada expressão regular é uma
  linguagem livre de contexto, podemos indagar "por que usar expressões
  regulares para definir a estrutura léxica da linguagem"? Existem várias
  razões:
  1. As regras léxicas de uma linguagem são frequentemente simples e para
  descrevê-las não precisamos de uma notação tão poderosa quanto a das
  gramáticas.
  2. As expressões regulares geralmente providenciam, para os tokens da
  gramática, uma notação mais concisa e facilmente compreendida.
  3. A partir de expressões regulares, podem ser construídos automaticamente
  analisadores léxicos mais eficientes do que a partir de gramáticas
  arbitrárias.
  4. A separação da estrutura sintática da linguagem nas partes léxica e
  não-léxica providencia uma forma conveniente de modularizar a vanguarda de um
  compilador em componentes administravelmente dimensionados.

  - Verificando a Linguagem Gerada por uma Gramática
  Apesar de os projetistas de compiladores raramente o fazerem para uma
  gramática de uma linguagem de programação completa, é importante estarmos
  capacitados a sustentar que um dado conjunto de produções gere uma linguagem
  particular. As construções problemáticas podem ser estudadas escrevendo-se
  uma gramática abstrata concisa e em seguida analisando-se a linguagem que a
  mesma gera. Uma prova de que uma gramática G gera uma linguagem L possui
  duas partes: precisamos mostrar que cada cadeia gerada por G está em L e,
  reciprocamente, que cada cadeia em L pode ser gerada por G.

  4.4 Análise Sintática Top-Down
  Definimos a classe de gramáticas LL a partir da qual os analisadores 
  preditivos podem ser construídos automaticamente.

  - Análise Sinática de Descendência Recursiva
  A análise sintática top-down pode ser vista como uma tentativa de se encontrar
  uma derivação mais à esquerda para uma cadeia de entrada. Equivalentemente,
  pode ser vista como uma tentativa de se construir uma árvore gramatical, para
  a cadeia de entrada, a partir da raiz, criando os nós da árvore gramatical em
  pré-ordem. Consideramos agora uma forma geral de análise sintática top-down,
  chamada de descendência recursiva, que pode envolver retrocesso, ou seja, a
  realização de esquadrinhamentos repeditos da entrada.

  - Analisadores Sintáticos Preditivos
  Em muitos, casos, escrevendo-se cuidadosamente uma gramática, eliminando-se a
  recursão à esquerda e fatorando-se à esquerda a gramática resultante, podemos
  obter uma nova gramática processálve por um analisador sintático de
  descendência recursiva que não necessite de retrocesso, isto é, um analisador
  sintático preditivo. Para construir um analisador sintátito preditivo,
  precisamos conhecer, dado o símbolo correte de entrar a e o não-terminal A a
  ser expandido, qual das alternativas da produção A -> a1 | a2 | ... | an é a
  única que deriva uma cadeia começando por a. Ou seja, a alternativa adequada
  precisa ser detectável examinando-se apenas para o primeiro símbolo da cadeia
  que a mesma deriva. As construções de controle de fluxo na maioria das
  linguagens de programação, com suas palavras-chave distintivas, são usualmente
  detectáveis dessa forma.
